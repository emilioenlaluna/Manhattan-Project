# -*- coding: utf-8 -*-
"""WEBSPRAPPING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q5t27qsTnkD5gRYOZBV2A6yI3TR0cdQS
"""

from os import path
from pathlib import PurePath
import requests
from bs4 import BeautifulSoup
import requests

with open('urls.txt', 'r') as fh:
    urls = fh.readlines()
urls = [url.strip() for url in urls]  # strip `\n`

for url in urls:
    print(url)
    file_name = PurePath(url).name
    file_path = path.join('.', file_name)
    text = ''
    response = requests.get(str(url))
    print(response.status_code)
    soup = BeautifulSoup(response.content, 'html.parser')
    h1=soup.find('h1')
    my_text = h1.text
    print(h1.text)
    my_div = soup.find('div', {'id': 'statya'})
    my_p_tags = my_div.find_all('p')
    # print the text content of each `p` element
    for p in my_p_tags:
        print(p.get_text())

import requests
from bs4 import BeautifulSoup

url = 'https://leyes-mx.com/codigo_penal_aguascalientes/2.htm'

response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')


h1=soup.find('h1')
my_text = h1.text
print(h1.text)

my_div = soup.find('div', {'id': 'statya'})

my_p_tags = my_div.find_all('p')

# print the text content of each `p` element
for p in my_p_tags:
    print(p.get_text())

from os import path
from pathlib import PurePath
import requests
from bs4 import BeautifulSoup

with open('urls.txt', 'r') as fh:
    urls = fh.readlines()
urls = [url.strip() for url in urls]  # strip `\n`

for url in urls:
    print(url)
    file_name = PurePath(url).name
    file_path = path.join('.', file_name)
    text = ''
    response = requests.get(str(url))
    print(response.status_code)
    soup = BeautifulSoup(response.content, 'html.parser')
    h1 = soup.find('h1')
    my_text = h1.text if h1 else None
    print(my_text)
    my_div = soup.find('div', {'id': 'statya'})
    my_p_tags = my_div.find_all('p') if my_div else []
    # print the text content of each `p` element
    for p in my_p_tags:
        print(p.get_text())

import csv
from os import path
from pathlib import PurePath
import requests
from bs4 import BeautifulSoup

# define the header row for the CSV file
header = ['h1', 'p']

# open the output file in write mode and create a CSV writer object
with open('output.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    
    # write the header row to the CSV file
    writer.writerow(header)
    
    with open('urls.txt', 'r') as fh:
        urls = fh.readlines()
    urls = [url.strip() for url in urls]  # strip `\n`

    for url in urls:
        print(url)
        file_name = PurePath(url).name
        file_path = path.join('.', file_name)
        response = requests.get(str(url))
        print(response.status_code)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # extract the text content of the `h1` tag
        h1 = soup.find('h1')
        h1_text = h1.text if h1 else None
        
        # extract the text content of all the `p` tags inside the `div` element with `id="statya"`
        my_div = soup.find('div', {'id': 'statya'})
        my_p_tags = my_div.find_all('p') if my_div else []
        p_texts = [p.get_text() for p in my_p_tags]
        
        # write the extracted data to the CSV file
        writer.writerow([h1_text, '\n'.join(p_texts)])